{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQEWtbLY3-xd"
      },
      "source": [
        "# Large Language Models\n",
        "\n",
        "In the ever-evolving landscape of artificial intelligence, large language models (LLMs) have emerged as transformative tools, reshaping the way we engage with and analyse language. These sophisticated models, honed on massive repositories of text data, possess the remarkable ability to comprehend, generate, and translate human language with unprecedented accuracy and fluency. Among the prominent LLM architectures, LangChain stands out for its efficiency and flexibility.\n",
        "\n",
        "This notebook is designed to seamlessly run both locally and on Google Colab. For those who may only have a CPU, there are clear instructions on how to run the notebook without a GPU. Don't worry, simply follow the instructions for either GPU or CPU, depending on your setup.\n",
        "\n",
        "Please note that using only a CPU will result in noticeably slower model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings 🛠️\n",
        "\n",
        "On Google Colab, you have access to free GPUs, whenever they're available. Let's utilise this advantage. To configure a Colab GPU, navigate to \"Edit\" and then \"Notebook Settings\". Select \"GPU\" and then click \"Save\".\n",
        "\n",
        "To proceed, you'll need to install two libraries: Langchain and Llama.cpp. When operating this notebook locally, you only need to install these libraries once, and they'll remain on your computer. However, in Colab, they're not default libraries and must be installed for each session.\n",
        "\n",
        "**LangChain** is a framework that simplifies the development of applications powered by large language models (LLMs)\n",
        "\n",
        "**llama.cpp** enables us to execute quantised versions of models.\n",
        "\n",
        "> Quantisation of LLMs is a process that reduces the precision of the numerical values in the model, such as converting 32-bit floating-point numbers to 8-bit integers. These models are therefore smaller and faster, allowing them to run on less powerful hardware with only a small loss in precision.\n",
        "\n",
        "* If you're using a **CPU**, use the [standard installation](https://python.langchain.com/docs/integrations/llms/llamacpp#cpu-only-installation) of llama.cpp. Windows users might have to install [a couple of extra libraries too](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-windows). Some students using windows have also found [this guide](https://medium.com/@piyushbatra1999/installing-llama-cpp-python-with-nvidia-gpu-acceleration-on-windows-a-short-guide-0dfac475002d) useful.\n",
        "* If you have an **NVIDIA GPU**, you need to [activate cuBLAS](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-openblas-cublas-clblast) with llama.cpp. cuBLAS is a library that speeds up operations on NVIDIA GPUs.\n",
        "* If you have a **silicon chip Apple with a GPU**, you need to [enable Metal](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-metal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_oTkGwppsp",
        "outputId": "f7bb55df-79de-4a3b-802d-53a0d4ca4b1a"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIolHmA0qIpT"
      },
      "source": [
        "Before we dive into the examples, let's download the large language model (LLM) we'll be using. For these exercises, we've selected a [quantised version of Mistral AI's Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). While this is a great choice, it's by no means the only option. We encourage you to explore and try different models to discover the unique strengths and weaknesses of each. Even models of similar size can exhibit surprisingly different capabilities.\n",
        "\n",
        "> Since we're working in Colab, we'll need to download the LLM for each session.\n",
        "<br>\n",
        "If you're working locally, you can download the model once. The model is then on your computer and doesn't need to be downloaded each time. Change the `--local-dir` to your folder of choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install huggingface-hub\n",
        "#[run it in the terminal]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run it in the terminal\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM 🧠\n",
        "\n",
        "Langchain simplifies LLM deployment with its streamlined setup process. A single line of code configures your LLM, allowing you to tailor the parameters to your specific needs.\n",
        "\n",
        "If you want to know more about Llama.cpp, you can [read the docs here](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/). Alternatively, here are the [LangChain docs for Llama.cpp](https://python.langchain.com/docs/integrations/llms/llamacpp).\n",
        "\n",
        "Here's a brief overview of some of the parameters:\n",
        "* **model_path:** The path to the Llama model file that will be used for generating text.\n",
        "* **max_tokens:** The maximum number of tokens that the model should generate in its response.\n",
        "* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n",
        "* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n",
        "* **n_gpu_layers:** The default setting of 0 will cause all layers to be executed on the CPU. Setting n_gpu_layers to 1 will cause the first layer of the model to be executed on the GPU, while the remaining layers are executed on the CPU. Setting n_gpu_layers to 2 will cause the first two layers of the model to be executed on the GPU, while the remaining layers are executed on the CPU, and so on. -1 will cause all layers to be offloaded to the GPU. In general, it is a good idea to experiment with different values of n_gpu_layers to find the best balance between performance and memory usage for your specific application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F76VzycqhLv",
        "outputId": "80ba035a-668d-4f07-b086-2191dad9b44c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.08 MiB, ( 4095.14 / 10922.67)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2\n",
            "ggml_metal_init: picking default device: Apple M2\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction support   = true\n",
            "ggml_metal_init: simdgroup matrix mul. support = true\n",
            "ggml_metal_init: hasUnifiedMemory              = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, ( 4160.95 / 10922.67)\n",
            "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     1.28 MiB, ( 4162.23 / 10922.67)\n",
            "llama_new_context_with_model:      Metal compute buffer size =     1.27 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     0.14 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxhMbwWZjyOm"
      },
      "source": [
        "If you're using a GPU, check the output of this cell ☝️\n",
        "  * If you're using cuBLAS, you'll see `BLAS = 1` if it's installed correctly.\n",
        "  * If you're using Metal, you'll see `NEON = 1` if it's installed correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG2XPn15NgV"
      },
      "source": [
        "---\n",
        "## 3.&nbsp; Asking your LLM questions 🤖\n",
        "Play around and note how small changes make a big difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCFyGekD1YWQ",
        "outputId": "168512bc-aecb-4239-de70-fbe657eec97e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    6205.29 ms\n",
            "llama_print_timings:      sample time =     106.10 ms /   503 runs   (    0.21 ms per token,  4740.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6205.24 ms /     8 tokens (  775.65 ms per token,     1.29 tokens per second)\n",
            "llama_print_timings:        eval time =   27044.97 ms /   503 runs   (   53.77 ms per token,    18.60 tokens per second)\n",
            "llama_print_timings:       total time =   34857.13 ms /   511 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Polar Bears\n",
            "2. Arctic Foxes\n",
            "3. Walruses\n",
            "4. Caribou\n",
            "5. Beluga Whales\n",
            "6. Narwhals\n",
            "7. Seals\n",
            "8. Musk Oxen\n",
            "9. Arctic Hares\n",
            "10. Snowy Owls\n",
            "11. Reindeer\n",
            "12. Beavers\n",
            "13. Moose\n",
            "14. Lynx\n",
            "15. Wolverines\n",
            "16. Caribou\n",
            "17. Arctic Hares\n",
            "18. Beluga Whales\n",
            "19. Narwhals\n",
            "20. Seals\n",
            "21. Musk Oxen\n",
            "22. Arctic Foxes\n",
            "23. Polar Bears\n",
            "24. Walruses\n",
            "25. Caribou\n",
            "26. Beluga Whales\n",
            "27. Narwhals\n",
            "28. Seals\n",
            "29. Musk Oxen\n",
            "30. Arctic Hares\n",
            "31. Snowy Owls\n",
            "32. Reindeer\n",
            "33. Beavers\n",
            "34. Moose\n",
            "35. Lynx\n",
            "36. Wolverines\n",
            "37. Caribou\n",
            "38. Arctic Hares\n",
            "39. Beluga Whales\n",
            "40. Narwhals\n",
            "41. Seals\n",
            "42. Musk Oxen\n",
            "43. Arctic Foxes\n",
            "44. Polar Bears\n",
            "45. Walruses\n",
            "46. Caribou\n",
            "47. Beluga Whales\n",
            "48. Narwhals\n",
            "49. Seals\n",
            "50. Musk Oxen\n",
            "51. Arctic Hares\n",
            "52. Snowy Owls\n",
            "53. Reindeer\n",
            "54. Beavers\n",
            "55. Moose\n",
            "56. Lynx\n",
            "57. Wolverines\n",
            "58. Caribou\n",
            "59. Arctic Hares\n",
            "60. Beluga Whales\n",
            "61. Narwhals\n",
            "62. Seals\n",
            "63. Musk Oxen\n",
            "64. Arctic Foxes\n",
            "65. Polar Bears\n",
            "66. Walruses\n",
            "67. Caribou\n",
            "68. Beluga Whales\n",
            "69. Narwhals\n",
            "70. Seals\n",
            "71. Musk Oxen\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"Which animals live at the north pole?\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8mzzu51d5b",
        "outputId": "cb182764-7990-49d3-a5b5-733b9d91ce5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6205.29 ms\n",
            "llama_print_timings:      sample time =      31.56 ms /   114 runs   (    0.28 ms per token,  3611.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     312.07 ms /     8 tokens (   39.01 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:        eval time =    6037.91 ms /   113 runs   (   53.43 ms per token,    18.72 tokens per second)\n",
            "llama_print_timings:       total time =    6805.22 ms /   121 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "?\n",
            "A: Generative AI refers to a type of artificial intelligence that can create new content, ideas or solutions based on existing data. It uses machine learning algorithms to analyze and understand patterns in the data, and then generates new outputs that are similar but not identical to the original inputs. This can include things like generating text, images, music, videos, and even entire websites or applications. Generative AI is often used in creative industries such as advertising, marketing, entertainment, and design, where it can help automate repetitive tasks and generate new ideas.\n"
          ]
        }
      ],
      "source": [
        "answer_2 = llm.invoke(\"what do you mean by Generative AI\")\n",
        "print(answer_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3LAlLON1G3g",
        "outputId": "ed3174f7-2282-453f-ae96-40f4a3b0f7c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6205.29 ms\n",
            "llama_print_timings:      sample time =      28.49 ms /    73 runs   (    0.39 ms per token,  2562.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     538.94 ms /    16 tokens (   33.68 ms per token,    29.69 tokens per second)\n",
            "llama_print_timings:        eval time =    3904.04 ms /    73 runs   (   53.48 ms per token,    18.70 tokens per second)\n",
            "llama_print_timings:       total time =    4857.62 ms /    89 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hugging Face is a company that makes software for computers to understand human language. They have a special tool called \"huggingface-cli\" that helps people use their software on their own computers. It's like having your own personal assistant that can help you with things like translating words from one language to another or finding information online.\n"
          ]
        }
      ],
      "source": [
        "answer_3 = llm.invoke(\"Explain the huggingface-cli like I'm 5 years old.\")\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnvnmmRIYQhF"
      },
      "source": [
        "The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well. These models may not have the most extensive knowledge base, but for our purposes, we only need them to generate coherent English. We'll then infuse them with specialised knowledge on a topic of your choice, resulting in a local, specialised model that can function offline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD4i8-3FEp04"
      },
      "source": [
        "---\n",
        "## 4.&nbsp; Challenge 😀\n",
        "Play around with this, and other, LLMs. keep a record of your findings:\n",
        "1. Pose different questions to the model, each subtly different from the last. Observe the resulting outputs. Smaller models tend to be highly sensitive to minor changes in language and grammar.\n",
        "2. Experiment with the parameters, one at a time, to assess their impact on the output.\n",
        "3. Attempt to load different models: Explore the [models page on HuggingFace](https://huggingface.co/models). You can use the left hand menu to find `Text Generation` under `Natural Language Processing`. Then use the filter bar for `GGUF` to find already quantised models.\n",
        "\n",
        "You can alter the download command accordingly. In this note book we used the command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzkwz4cI1DgP"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCADTg0j1xnU"
      },
      "source": [
        "This downloads the version `mistral-7b-instruct-v0.1.Q4_K_M.gguf` of the model `TheBloke/Mistral-7B-Instruct-v0.1-GGUF` from huggingface. You can read about the different versions on the models `model card`.\n",
        "\n",
        "To adapt this just change the model and the version to your new choice.\n",
        "\n",
        "`!huggingface-cli download {model_name} {model_version} --local-dir . --local-dir-use-symlinks False`\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ChvulP1Dd0"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3820.94 MiB, ( 7983.17 / 10922.67)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      Metal buffer size =  3820.93 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2\n",
            "ggml_metal_init: picking default device: Apple M2\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction support   = true\n",
            "ggml_metal_init: simdgroup matrix mul. support = true\n",
            "ggml_metal_init: hasUnifiedMemory              = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 8240.17 / 10922.67)\n",
            "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     1.11 MiB, ( 8241.28 / 10922.67)\n",
            "llama_new_context_with_model:      Metal compute buffer size =     1.10 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     0.14 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "#llama-2-7b-chat.Q4_K_M.gguf\n",
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm1 = LlamaCpp(model_path = \"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    5573.47 ms\n",
            "llama_print_timings:      sample time =     112.51 ms /   501 runs   (    0.22 ms per token,  4453.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5783.98 ms /    11 tokens (  525.82 ms per token,     1.90 tokens per second)\n",
            "llama_print_timings:        eval time =   26561.44 ms /   500 runs   (   53.12 ms per token,    18.82 tokens per second)\n",
            "llama_print_timings:       total time =   33938.72 ms /   511 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "I am interested in pursuing a career as a data analyst. How do I go about getting a job in this field? What are some of the key skills and qualifications that employers typically look for in a data analyst candidate?\n",
            "Answer: Pursuing a career as a data analyst can be an exciting and rewarding choice, as organizations across various industries increasingly rely on data-driven decision making. Here are some steps you can take to increase your chances of landing a job as a data analyst:\n",
            "1. Build a strong foundation in statistics and mathematics: Data analysis involves working with large datasets, identifying patterns, and drawing meaningful insights. Therefore, it is essential to have a good grasp of statistical concepts such as regression analysis, hypothesis testing, and time series analysis. Additionally, proficiency in mathematical concepts like linear algebra and calculus can be helpful.\n",
            "2. Learn data visualization tools: Data analysts use various visualization tools to present their findings to stakeholders. Familiarity with tools like Tableau, Power BI, or D3.js can make you a more attractive candidate.\n",
            "3. Develop programming skills: Programming languages like Python, R, and SQL are commonly used in data analysis. Learning these languages can help you perform tasks such as data cleaning, transformation, and analysis more efficiently.\n",
            "4. Gain experience through internships or side projects: Many employers prefer candidates with practical experience in data analysis. Consider taking on an internship or working on personal projects that involve data analysis to build your portfolio.\n",
            "5. Network with professionals in the field: Attend industry events, join professional organizations, or connect with data analysts through LinkedIn. Building relationships with experienced professionals can help you stay up-to-date on industry trends and learn about job opportunities before they are advertised publicly.\n",
            "6. Develop soft skills: Data analysis is a collaborative process that often involves working with cross-functional teams and stakeholders. Therefore, it's essential to have good communication, collaboration, and problem-solving skills.\n",
            "7. Pursue certifications or degrees in data science: While not always required, obtaining a degree in data science or a related field can make you a more competitive candidate. Similarly, pursuing certifications like the Certified Data Analyst (\n"
          ]
        }
      ],
      "source": [
        "answer_4 = llm1.invoke('how can I get a data analyst job?')\n",
        "print(answer_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IouC-hN42PHc"
      },
      "source": [
        "The code above would download a [quantised version of Meta's Llama 2 7B chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
