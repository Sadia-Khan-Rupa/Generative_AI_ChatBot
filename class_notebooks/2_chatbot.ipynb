{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQEWtbLY3-xd"
      },
      "source": [
        "# Chatbot\n",
        "In this tutorial, we'll be designing a chatbot with the capability to retain information from previous prompts and responses, enabling it to maintain context throughout the conversation. This ability sets it apart from LLMs, which typically process language in a more static manner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings ðŸ› ï¸\n",
        "Let's replicate the process of downloading and installing the necessary libraries and model, just like we did in the previous LLM notebook. If you had any specific settings in the first notebook, such as using a CPU instead of a GPU, maintain those same configurations here.\n",
        "\n",
        "Since we'll be continuing our work on Colab with a GPU, let's activate it by navigating to `Edit > Notebook Settings`. Select `GPU` and click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_oTkGwppsp",
        "outputId": "e5402acf-ae31-4af4-d26f-9be5c3bf9292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 516, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 631, in _prepare_linked_requirement\n",
            "    dist = _get_prepared_distribution(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 69, in _get_prepared_distribution\n",
            "    abstract_dist.prepare_distribution_metadata(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/distributions/sdist.py\", line 38, in prepare_distribution_metadata\n",
            "    self._prepare_build_backend(finder)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/distributions/sdist.py\", line 70, in _prepare_build_backend\n",
            "    self.req.build_env.install_requirements(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 217, in install_requirements\n",
            "    self._install_requirements(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 275, in _install_requirements\n",
            "    call_subprocess(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 166, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 207, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 370, in extract\n",
            "    linecache.lazycache(filename, f.f_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 165, in lazycache\n",
            "    if not filename or (filename.startswith('<') and filename.endswith('>')):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp7otza3u9\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf:  70% 3.06G/4.37G [00:46<00:14, 87.4MB/s]"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM ðŸ§ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-F76VzycqhLv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.08 MiB, ( 4095.14 / 10922.67)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2\n",
            "ggml_metal_init: picking default device: Apple M2\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction support   = true\n",
            "ggml_metal_init: simdgroup matrix mul. support = true\n",
            "ggml_metal_init: hasUnifiedMemory              = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, ( 4160.95 / 10922.67)\n",
            "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     1.28 MiB, ( 4162.23 / 10922.67)\n",
            "llama_new_context_with_model:      Metal compute buffer size =     1.27 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     0.14 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG2XPn15NgV"
      },
      "source": [
        "### 2.1.&nbsp; Test your LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fc8mzzu51d5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =       9.42 ms /   104 runs   (    0.09 ms per token, 11036.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6777.09 ms /    14 tokens (  484.08 ms per token,     2.07 tokens per second)\n",
            "llama_print_timings:        eval time =    5305.27 ms /   103 runs   (   51.51 ms per token,    19.41 tokens per second)\n",
            "llama_print_timings:       total time =   12219.11 ms /   117 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "I am a 25 year old female who has been struggling with my self esteem for as long as I can remember. I have always had a hard time loving myself and accepting myself for who I am. I have struggled with body image issues, anxiety, depression, and low self worth. I have tried therapy, medication, and various self help techniques but nothing seems to work. How do I learn to love myself?\n",
            "\n",
            "Thank you in advance for any advice or guidance you may have.\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"HOW SHOULD I LOVE MYSELF.\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhplYxpo4sb1"
      },
      "source": [
        "---\n",
        "## 3.&nbsp; Making a chatbot ðŸ’¬\n",
        "To transform a basic LLM into a chatbot, we'll need to infuse it with additional functionalities: prompts, memory, and chains.\n",
        "\n",
        "**Prompts** are like the instructions you give the chatbot to tell it what to do. Whether you want it to write a poem, translate a language, or answer your questions. They provide the context and purpose for its responses.\n",
        "\n",
        "**Memory** is like the chatbot's brain. It stores information from previous interactions, allowing it to remember what you've said and keep conversations flowing naturally.\n",
        "\n",
        "The **chain** is like a road map that guides the conversation along the right path. It tells the LLM how to process your prompts, how to access the memory bank, and how to generate its responses.\n",
        "\n",
        "In essence, prompts provide the direction, memory retains the context, and chains orchestrate the interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-iud1c8L4s2R"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "### Prompt ###\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"Keep your answers very short, succinct, and to the point.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "### Memory ###\n",
        "# We `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# `\"chat_history\"` must align with the MessagesPlaceholder name\n",
        "memory = ConversationBufferMemory(memory_key = \"chat_history\",\n",
        "                                  return_messages=True)\n",
        "\n",
        "### Chain ###\n",
        "conversation = LLMChain(llm = llm,\n",
        "                        prompt = prompt,\n",
        "                        verbose = True,\n",
        "                        memory = memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yym6NjddEKN"
      },
      "source": [
        "### 3.1.&nbsp; Test your chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "umtqcoo-dC0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sadiakhanrupa/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =       1.97 ms /    13 runs   (    0.15 ms per token,  6592.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7029.50 ms /    21 tokens (  334.74 ms per token,     2.99 tokens per second)\n",
            "llama_print_timings:        eval time =     636.62 ms /    12 runs   (   53.05 ms per token,    18.85 tokens per second)\n",
            "llama_print_timings:       total time =    7700.71 ms /    33 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?')],\n",
              " 'text': '\\nAI: Hello! How can I assist you today?'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lsOUTUFdn0GO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: How can I love my husband more\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =      29.03 ms /    75 runs   (    0.39 ms per token,  2583.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     881.53 ms /    24 tokens (   36.73 ms per token,    27.23 tokens per second)\n",
            "llama_print_timings:        eval time =    3986.34 ms /    74 runs   (   53.87 ms per token,    18.56 tokens per second)\n",
            "llama_print_timings:       total time =    5271.22 ms /    98 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'How can I love my husband more',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='How can I love my husband more'),\n",
              "  AIMessage(content=\"?\\nAI: There are many ways to love your husband more. Some ideas include expressing gratitude for him, showing affection through physical touch and words of encouragement, spending quality time together, and actively listening to his needs and concerns. It's also important to communicate openly and honestly with him about your feelings and to be supportive of his goals and aspirations.\")],\n",
              " 'text': \"?\\nAI: There are many ways to love your husband more. Some ideas include expressing gratitude for him, showing affection through physical touch and words of encouragement, spending quality time together, and actively listening to his needs and concerns. It's also important to communicate openly and honestly with him about your feelings and to be supportive of his goals and aspirations.\"}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"How can I love my husband more\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7RbaU6QJoAO1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: How can I love my husband more\n",
            "AI: ?\n",
            "AI: There are many ways to love your husband more. Some ideas include expressing gratitude for him, showing affection through physical touch and words of encouragement, spending quality time together, and actively listening to his needs and concerns. It's also important to communicate openly and honestly with him about your feelings and to be supportive of his goals and aspirations.\n",
            "Human: it is a sunny day, should I make BBQ?\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =      31.14 ms /    82 runs   (    0.38 ms per token,  2633.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5778.13 ms /    94 tokens (   61.47 ms per token,    16.27 tokens per second)\n",
            "llama_print_timings:        eval time =    4416.99 ms /    81 runs   (   54.53 ms per token,    18.34 tokens per second)\n",
            "llama_print_timings:       total time =   10714.68 ms /   175 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'it is a sunny day, should I make BBQ?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='How can I love my husband more'),\n",
              "  AIMessage(content=\"?\\nAI: There are many ways to love your husband more. Some ideas include expressing gratitude for him, showing affection through physical touch and words of encouragement, spending quality time together, and actively listening to his needs and concerns. It's also important to communicate openly and honestly with him about your feelings and to be supportive of his goals and aspirations.\"),\n",
              "  HumanMessage(content='it is a sunny day, should I make BBQ?'),\n",
              "  AIMessage(content='\\nAI: ?\\nAI: Yes, it sounds like a great idea! Grilling can be a fun and delicious way to spend time outdoors on a sunny day. You could try making burgers, hot dogs, chicken, or even grilled vegetables for a healthy option. Just remember to keep an eye on the weather forecast and make sure you have enough food and drinks for everyone.')],\n",
              " 'text': '\\nAI: ?\\nAI: Yes, it sounds like a great idea! Grilling can be a fun and delicious way to spend time outdoors on a sunny day. You could try making burgers, hot dogs, chicken, or even grilled vegetables for a healthy option. Just remember to keep an eye on the weather forecast and make sure you have enough food and drinks for everyone.'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"it is a sunny day, should I make BBQ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y5PGH3Y4ohYo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: How can I love my husband more\n",
            "AI: ?\n",
            "AI: There are many ways to love your husband more. Some ideas include expressing gratitude for him, showing affection through physical touch and words of encouragement, spending quality time together, and actively listening to his needs and concerns. It's also important to communicate openly and honestly with him about your feelings and to be supportive of his goals and aspirations.\n",
            "Human: it is a sunny day, should I make BBQ?\n",
            "AI: \n",
            "AI: ?\n",
            "AI: Yes, it sounds like a great idea! Grilling can be a fun and delicious way to spend time outdoors on a sunny day. You could try making burgers, hot dogs, chicken, or even grilled vegetables for a healthy option. Just remember to keep an eye on the weather forecast and make sure you have enough food and drinks for everyone.\n",
            "Human: how is weather in HÃ¶stel, germany?\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =      23.36 ms /    56 runs   (    0.42 ms per token,  2397.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2799.78 ms /    96 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
            "llama_print_timings:        eval time =    3055.13 ms /    56 runs   (   54.56 ms per token,    18.33 tokens per second)\n",
            "llama_print_timings:       total time =    6233.13 ms /   152 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'how is weather in HÃ¶stel, germany?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='How can I love my husband more'),\n",
              "  AIMessage(content=\"?\\nAI: There are many ways to love your husband more. Some ideas include expressing gratitude for him, showing affection through physical touch and words of encouragement, spending quality time together, and actively listening to his needs and concerns. It's also important to communicate openly and honestly with him about your feelings and to be supportive of his goals and aspirations.\"),\n",
              "  HumanMessage(content='it is a sunny day, should I make BBQ?'),\n",
              "  AIMessage(content='\\nAI: ?\\nAI: Yes, it sounds like a great idea! Grilling can be a fun and delicious way to spend time outdoors on a sunny day. You could try making burgers, hot dogs, chicken, or even grilled vegetables for a healthy option. Just remember to keep an eye on the weather forecast and make sure you have enough food and drinks for everyone.'),\n",
              "  HumanMessage(content='how is weather in HÃ¶stel, germany?'),\n",
              "  AIMessage(content=\"\\nAI: \\nAI: ?\\nAI: I'm sorry, I don't have real-time information about the weather in HÃ¶stel, Germany. You can check a reliable weather website or app to get the most up-to-date information.\")],\n",
              " 'text': \"\\nAI: \\nAI: ?\\nAI: I'm sorry, I don't have real-time information about the weather in HÃ¶stel, Germany. You can check a reliable weather website or app to get the most up-to-date information.\"}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"how is weather in HÃ¶stel, germany?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyk600ivwAxd"
      },
      "source": [
        "---\n",
        "## 4.&nbsp; Challenge ðŸ˜€\n",
        "1. Play around with writing new prompts.\n",
        "  * Try having an empty prompt, what does it do to the output?\n",
        "  * Try having a funny prompt.\n",
        "  * Try having a long, precise prompt.\n",
        "  * Try all different kinds of prompts.\n",
        "2. Try out the different types of memory. What effect does this have on the type of conversations you have? Slower or faster? Better or worse accuracy? Forgetting things or super precise?\n",
        "  * [ConversationBufferWindowMemory](https://python.langchain.com/docs/modules/memory/types/buffer_window) - Only a set number of k remembered - e.g. last 6 inputs\n",
        "  * [ConversationSummaryMemory](https://python.langchain.com/docs/modules/memory/types/summary) - summarises the history to save space\n",
        "  * [ConversationSummaryBufferMemory](https://python.langchain.com/docs/modules/memory/types/summary_buffer) - summarises and restricted by token length\n",
        "3. Try different LLMs with different types of prompts and memory. Which combination works best for you? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aRsQ38ZS-3vU"
      },
      "outputs": [],
      "source": [
        "##empty prompt\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "### Prompt ###\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "          ' ' \n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "### Memory ###\n",
        "# We `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# `\"chat_history\"` must align with the MessagesPlaceholder name\n",
        "memory = ConversationBufferMemory(memory_key = \"chat_history\",\n",
        "                                  return_messages=True)\n",
        "\n",
        "### Chain ###\n",
        "conversation1 = LLMChain(llm = llm,\n",
        "                        prompt = prompt,\n",
        "                        verbose = True,\n",
        "                        memory = memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem:  \n",
            "Human: hi\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /    13 runs   (    0.09 ms per token, 11149.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     359.89 ms /     6 tokens (   59.98 ms per token,    16.67 tokens per second)\n",
            "llama_print_timings:        eval time =     626.10 ms /    12 runs   (   52.17 ms per token,    19.17 tokens per second)\n",
            "llama_print_timings:       total time =    1006.36 ms /    18 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?')],\n",
              " 'text': '\\nAI: Hello! How can I assist you today?'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation1({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem:  \n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: How can I update my LinkedIn?\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =      49.78 ms /   112 runs   (    0.44 ms per token,  2249.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     774.48 ms /    24 tokens (   32.27 ms per token,    30.99 tokens per second)\n",
            "llama_print_timings:        eval time =    6027.00 ms /   112 runs   (   53.81 ms per token,    18.58 tokens per second)\n",
            "llama_print_timings:       total time =    7480.69 ms /   136 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'How can I update my LinkedIn?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='How can I update my LinkedIn?'),\n",
              "  AIMessage(content='\\nAI: To update your LinkedIn, you can follow these steps:\\n1. Log in to your LinkedIn account.\\n2. Click on the \"Profile\" tab.\\n3. Make any necessary updates to your profile information, such as your name, job title, and summary.\\n4. Upload a new profile picture if desired.\\n5. Preview your changes to make sure everything looks correct.\\n6. Click \"Save\" to update your LinkedIn profile.\\nIs there anything else I can help you with?')],\n",
              " 'text': '\\nAI: To update your LinkedIn, you can follow these steps:\\n1. Log in to your LinkedIn account.\\n2. Click on the \"Profile\" tab.\\n3. Make any necessary updates to your profile information, such as your name, job title, and summary.\\n4. Upload a new profile picture if desired.\\n5. Preview your changes to make sure everything looks correct.\\n6. Click \"Save\" to update your LinkedIn profile.\\nIs there anything else I can help you with?'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation1({\"question\": \"How can I update my LinkedIn?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem:  \n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: How can I update my LinkedIn?\n",
            "AI: \n",
            "AI: To update your LinkedIn, you can follow these steps:\n",
            "1. Log in to your LinkedIn account.\n",
            "2. Click on the \"Profile\" tab.\n",
            "3. Make any necessary updates to your profile information, such as your name, job title, and summary.\n",
            "4. Upload a new profile picture if desired.\n",
            "5. Preview your changes to make sure everything looks correct.\n",
            "6. Click \"Save\" to update your LinkedIn profile.\n",
            "Is there anything else I can help you with?\n",
            "Human: How Can I use it zo get data analyst job?\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =      13.90 ms /   158 runs   (    0.09 ms per token, 11367.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3670.70 ms /   127 tokens (   28.90 ms per token,    34.60 tokens per second)\n",
            "llama_print_timings:        eval time =    8412.02 ms /   157 runs   (   53.58 ms per token,    18.66 tokens per second)\n",
            "llama_print_timings:       total time =   12313.80 ms /   284 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'How Can I use it zo get data analyst job?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='How can I update my LinkedIn?'),\n",
              "  AIMessage(content='\\nAI: To update your LinkedIn, you can follow these steps:\\n1. Log in to your LinkedIn account.\\n2. Click on the \"Profile\" tab.\\n3. Make any necessary updates to your profile information, such as your name, job title, and summary.\\n4. Upload a new profile picture if desired.\\n5. Preview your changes to make sure everything looks correct.\\n6. Click \"Save\" to update your LinkedIn profile.\\nIs there anything else I can help you with?'),\n",
              "  HumanMessage(content='How Can I use it zo get data analyst job?'),\n",
              "  AIMessage(content=\"\\nAI: \\nAI: To use LinkedIn to find a data analyst job, you can follow these steps:\\n1. Optimize your LinkedIn profile by highlighting your relevant skills and experience in data analysis.\\n2. Connect with other professionals in the data analytics field and join relevant groups.\\n3. Use LinkedIn's job search feature to find open positions that match your qualifications.\\n4. Apply for jobs directly through LinkedIn or by clicking on the job link provided.\\n5. Follow up with recruiters and hiring managers to express your interest in the position.\\n6. Keep your LinkedIn profile up-to-date to showcase your progress and achievements as a data analyst.\\nIs there anything else I can help you with?\")],\n",
              " 'text': \"\\nAI: \\nAI: To use LinkedIn to find a data analyst job, you can follow these steps:\\n1. Optimize your LinkedIn profile by highlighting your relevant skills and experience in data analysis.\\n2. Connect with other professionals in the data analytics field and join relevant groups.\\n3. Use LinkedIn's job search feature to find open positions that match your qualifications.\\n4. Apply for jobs directly through LinkedIn or by clicking on the job link provided.\\n5. Follow up with recruiters and hiring managers to express your interest in the position.\\n6. Keep your LinkedIn profile up-to-date to showcase your progress and achievements as a data analyst.\\nIs there anything else I can help you with?\"}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " conversation1({\"question\": \"How Can I use it zo get data analyst job?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem:  \n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: How can I update my LinkedIn?\n",
            "AI: \n",
            "AI: To update your LinkedIn, you can follow these steps:\n",
            "1. Log in to your LinkedIn account.\n",
            "2. Click on the \"Profile\" tab.\n",
            "3. Make any necessary updates to your profile information, such as your name, job title, and summary.\n",
            "4. Upload a new profile picture if desired.\n",
            "5. Preview your changes to make sure everything looks correct.\n",
            "6. Click \"Save\" to update your LinkedIn profile.\n",
            "Is there anything else I can help you with?\n",
            "Human: How Can I use it zo get data analyst job?\n",
            "AI: \n",
            "AI: \n",
            "AI: To use LinkedIn to find a data analyst job, you can follow these steps:\n",
            "1. Optimize your LinkedIn profile by highlighting your relevant skills and experience in data analysis.\n",
            "2. Connect with other professionals in the data analytics field and join relevant groups.\n",
            "3. Use LinkedIn's job search feature to find open positions that match your qualifications.\n",
            "4. Apply for jobs directly through LinkedIn or by clicking on the job link provided.\n",
            "5. Follow up with recruiters and hiring managers to express your interest in the position.\n",
            "6. Keep your LinkedIn profile up-to-date to showcase your progress and achievements as a data analyst.\n",
            "Is there anything else I can help you with?\n",
            "Human: Why having a mentor is important for this job roles\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6552.10 ms\n",
            "llama_print_timings:      sample time =      71.55 ms /   169 runs   (    0.42 ms per token,  2362.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5367.38 ms /     9 tokens (  596.38 ms per token,     1.68 tokens per second)\n",
            "llama_print_timings:        eval time =    9355.92 ms /   168 runs   (   55.69 ms per token,    17.96 tokens per second)\n",
            "llama_print_timings:       total time =   10615.14 ms /   177 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Why having a mentor is important for this job roles',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='How can I update my LinkedIn?'),\n",
              "  AIMessage(content='\\nAI: To update your LinkedIn, you can follow these steps:\\n1. Log in to your LinkedIn account.\\n2. Click on the \"Profile\" tab.\\n3. Make any necessary updates to your profile information, such as your name, job title, and summary.\\n4. Upload a new profile picture if desired.\\n5. Preview your changes to make sure everything looks correct.\\n6. Click \"Save\" to update your LinkedIn profile.\\nIs there anything else I can help you with?'),\n",
              "  HumanMessage(content='How Can I use it zo get data analyst job?'),\n",
              "  AIMessage(content=\"\\nAI: \\nAI: To use LinkedIn to find a data analyst job, you can follow these steps:\\n1. Optimize your LinkedIn profile by highlighting your relevant skills and experience in data analysis.\\n2. Connect with other professionals in the data analytics field and join relevant groups.\\n3. Use LinkedIn's job search feature to find open positions that match your qualifications.\\n4. Apply for jobs directly through LinkedIn or by clicking on the job link provided.\\n5. Follow up with recruiters and hiring managers to express your interest in the position.\\n6. Keep your LinkedIn profile up-to-date to showcase your progress and achievements as a data analyst.\\nIs there anything else I can help you with?\"),\n",
              "  HumanMessage(content='Why having a mentor is important for this job roles'),\n",
              "  AIMessage(content='?\\nAI: \\nAI: Having a mentor in the data analytics field can be very beneficial for several reasons:\\n1. Guidance and support: A mentor can provide guidance and support as you navigate your career, helping you make informed decisions and overcome challenges.\\n2. Networking opportunities: A mentor may have connections and networking opportunities that can help you advance in your career.\\n3. Professional development: A mentor can offer feedback and advice on how to improve your skills and knowledge in the data analytics field.\\n4. Personal growth: Working with a mentor can help you set goals and develop a plan for achieving them, leading to personal growth and fulfillment.\\nOverall, having a mentor in the data analytics field can be a valuable resource that can help you succeed in your career.')],\n",
              " 'text': '?\\nAI: \\nAI: Having a mentor in the data analytics field can be very beneficial for several reasons:\\n1. Guidance and support: A mentor can provide guidance and support as you navigate your career, helping you make informed decisions and overcome challenges.\\n2. Networking opportunities: A mentor may have connections and networking opportunities that can help you advance in your career.\\n3. Professional development: A mentor can offer feedback and advice on how to improve your skills and knowledge in the data analytics field.\\n4. Personal growth: Working with a mentor can help you set goals and develop a plan for achieving them, leading to personal growth and fulfillment.\\nOverall, having a mentor in the data analytics field can be a valuable resource that can help you succeed in your career.'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " conversation1({\"question\": \"Why having a mentor is important for this job roles\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
