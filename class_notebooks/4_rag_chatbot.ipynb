{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74fDzeoSRf2W"
      },
      "source": [
        "# Rag chatbot\n",
        "Let's take the next step in our journey! We've explored LLMs, chatbots, and RAG. Now, it's time to put them all together to create a powerful tool: a RAG chain with memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voDa-iHRRnfc"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings üõ†Ô∏è\n",
        "Except one item, this is the same as the last notebook. The only new item is a line to download a saved verion of the vector database created from Alice's Adventures in Wonderland. This saves us loading, splitting, and vectorising the book all over again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8WEhS5-Ktmc",
        "outputId": "e3f40e94-d096-4849-c623-a81eec7f3ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpzvgvot1y\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:44<00:00, 98.2MB/s]\n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZAQCytqZqVl",
        "outputId": "1b89f9cf-9728-400f-e392-567a2f995522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-5.1.0\n",
            "Retrieving folder contents\n",
            "Processing file 1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt index.faiss\n",
            "Processing file 1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ index.pkl\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt\n",
            "To: /content/faiss_index/index.faiss\n",
            "100% 421k/421k [00:00<00:00, 2.96MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ\n",
            "To: /content/faiss_index/index.pkl\n",
            "100% 216k/216k [00:00<00:00, 2.14MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# in case you get the error 'NoneType' object has no attribute 'groups'\n",
        "!pip install --upgrade gdown\n",
        "\n",
        "# download saved vector database for Alice's Adventures in Wonderland\n",
        "!gdown --folder 1A8A9lhcUXUKRrtCe7rckMlQtgmfLZRQH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A61c2eNVTnyG"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up the chain üîó\n",
        "There are 2 new items in this code that you haven't seen before:\n",
        "* the `output_key` parameter in [ConversationBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html)\n",
        "* [ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#)\n",
        "\n",
        "The `ConversationalRetrievalChain` is the LangChain chain for RAG with memory.\n",
        "\n",
        "The `output_key` parameter is necessary if you want to include both `memory` and `return_source_documents` with `ConversationalRetrievalChain`. Without this parameter in `memory`, you'll get an error in `ConversationalRetrievalChain` when using both the aformentioned parameters as it gets two parameters when it's expecting one. Adding this parameter to `memory` means it knows which output to accept. If you're not using both `memory` and `return_source_documents` with `ConversationalRetrievalChain`, this isn't necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3PlNxW1Jh5i",
        "outputId": "fe111b82-fdf4-410b-9223-f568a5981cfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.08 MiB, ( 4095.14 / 10922.67)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 1024\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2\n",
            "ggml_metal_init: picking default device: Apple M2\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction support   = true\n",
            "ggml_metal_init: simdgroup matrix mul. support = true\n",
            "ggml_metal_init: hasUnifiedMemory              = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 4224.95 / 10922.67)\n",
            "llama_kv_cache_init:      Metal KV buffer size =   128.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     1.55 MiB, ( 4226.50 / 10922.67)\n",
            "llama_new_context_with_model:      Metal compute buffer size =     1.53 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     0.16 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# llm\n",
        "llm = LlamaCpp(model_path = \"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1,\n",
        "               n_ctx = 1024)\n",
        "\n",
        "# embeddings\n",
        "import os\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "current_folder = os.getcwd()\n",
        "embeddings_folder = current_folder  # Using the current directory\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model, cache_folder=embeddings_folder)\n",
        "\n",
        "# load vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/vector\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# memory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history',\n",
        "                                  return_messages=True,\n",
        "                                  output_key='answer')\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
        "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
        "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
        "  (3) Ensure your answer answers the question, is helpful, professional, and formatted to be easily readable.\n",
        "  (4) Give funniest answers\n",
        "[/INST]\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]\n",
        "Helpful Answer:\n",
        "[INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              retriever=retriever,\n",
        "                                              memory=memory,\n",
        "                                              return_source_documents=True,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGWmLR5RO-Na",
        "outputId": "766148ed-b425-41f3-99d7-069e49c11fac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    3255.06 ms\n",
            "llama_print_timings:      sample time =      35.26 ms /    87 runs   (    0.41 ms per token,  2467.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18584.95 ms /   531 tokens (   35.00 ms per token,    28.57 tokens per second)\n",
            "llama_print_timings:        eval time =    5359.66 ms /    86 runs   (   62.32 ms per token,    16.05 tokens per second)\n",
            "llama_print_timings:       total time =   24505.94 ms /   617 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Who is the queen?',\n",
              " 'chat_history': [HumanMessage(content='Who is the queen?'),\n",
              "  AIMessage(content='The Queen in the story is the Queen of Hearts. She is a character in Lewis Carroll\\'s \"Alice\\'s Adventures in Wonderland\". In the story, she is portrayed as a tyrannical ruler who is quick to anger and has a penchant for beheadings. The Queen is also known for her love of playing croquet and her dislike of hedgehogs.')],\n",
              " 'answer': 'The Queen in the story is the Queen of Hearts. She is a character in Lewis Carroll\\'s \"Alice\\'s Adventures in Wonderland\". In the story, she is portrayed as a tyrannical ruler who is quick to anger and has a penchant for beheadings. The Queen is also known for her love of playing croquet and her dislike of hedgehogs.',\n",
              " 'source_documents': [Document(page_content='‚ÄúWould you tell me,‚Äù said Alice, a little timidly, ‚Äúwhy you are\\npainting those roses?‚Äù\\n\\nFive and Seven said nothing, but looked at Two. Two began in a low\\nvoice, ‚ÄúWhy the fact is, you see, Miss, this here ought to have been a\\n_red_ rose-tree, and we put a white one in by mistake; and if the Queen\\nwas to find it out, we should all have our heads cut off, you know. So\\nyou see, Miss, we‚Äôre doing our best, afore she comes, to‚Äî‚Äù At this\\nmoment Five, who had been anxiously looking across the garden, called\\nout ‚ÄúThe Queen! The Queen!‚Äù and the three gardeners instantly threw\\nthemselves flat upon their faces. There was a sound of many footsteps,\\nand Alice looked round, eager to see the Queen.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'}),\n",
              "  Document(page_content='by without noticing her. Then followed the Knave of Hearts, carrying\\nthe King‚Äôs crown on a crimson velvet cushion; and, last of all this\\ngrand procession, came THE KING AND QUEEN OF HEARTS.', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/alice_in_wonderland.txt'})]}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"Who is the queen?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuQsjOktO-Kg",
        "outputId": "1f29bc4a-bbc7-405f-fa5c-7f56441c69c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3255.06 ms\n",
            "llama_print_timings:      sample time =       7.52 ms /    20 runs   (    0.38 ms per token,  2661.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4343.00 ms /   149 tokens (   29.15 ms per token,    34.31 tokens per second)\n",
            "llama_print_timings:        eval time =    1021.27 ms /    19 runs   (   53.75 ms per token,    18.60 tokens per second)\n",
            "llama_print_timings:       total time =    5521.28 ms /   168 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3255.06 ms\n",
            "llama_print_timings:      sample time =      16.06 ms /    59 runs   (    0.27 ms per token,  3673.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20449.66 ms /   715 tokens (   28.60 ms per token,    34.96 tokens per second)\n",
            "llama_print_timings:        eval time =    3351.92 ms /    58 runs   (   57.79 ms per token,    17.30 tokens per second)\n",
            "llama_print_timings:       total time =   24320.08 ms /   773 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Queen in \"Alice's Adventures in Wonderland\" enjoys playing croquet with hedgehogs. She also likes to reminisce about her childhood and make her own children's eyes bright with strange tales, including the dream of Wonderland from long ago.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"What does she enjoy doing?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLN-pj1UO-Fn",
        "outputId": "dfb12cd9-7492-407e-b7c6-b0ecc9cfc170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3255.06 ms\n",
            "llama_print_timings:      sample time =       8.19 ms /    22 runs   (    0.37 ms per token,  2686.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6354.37 ms /   223 tokens (   28.49 ms per token,    35.09 tokens per second)\n",
            "llama_print_timings:        eval time =    1145.78 ms /    21 runs   (   54.56 ms per token,    18.33 tokens per second)\n",
            "llama_print_timings:       total time =    7702.24 ms /   244 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3255.06 ms\n",
            "llama_print_timings:      sample time =      10.10 ms /   124 runs   (    0.08 ms per token, 12279.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17664.93 ms /   618 tokens (   28.58 ms per token,    34.98 tokens per second)\n",
            "llama_print_timings:        eval time =    6861.93 ms /   123 runs   (   55.79 ms per token,    17.92 tokens per second)\n",
            "llama_print_timings:       total time =   24920.52 ms /   741 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Queen in \"Alice's Adventures in Wonderland\" chops off the head of the White Rabbit.\n",
            "\n",
            "In the story, the Queen is angry and frustrated with the players for not following her rules. She becomes even more enraged when she sees Alice playing with the hedgehogs instead of taking her turn. The Queen then orders the executioner to behead the players who have missed their turns, including the White Rabbit.\n",
            "\n",
            "It's important to note that this is a fictional story and the actions of the characters are not meant to be taken seriously.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"Whose head does she chop off?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFAIBPuHpOTn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
