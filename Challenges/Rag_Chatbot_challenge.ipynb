{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career Rag chatbot\n",
    "Let's take the next step in our journey! We've explored LLMs, chatbots, and RAG. Now, it's time to put them all together to create a powerful tool: a RAG chain with memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.&nbsp; Installations and Settings üõ†Ô∏è\n",
    "Except one item, this is the same as the last notebook. The only new item is a line to download a saved version of the vector database created from Alice's Adventures in Wonderland. This saves us loading, splitting, and vectorising the book all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qqq langchain --progress-bar off\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
    "!pip3 install -qqq sentence_transformers --progress-bar off\n",
    "!pip3 install -qqq faiss-gpu --progress-bar off\n",
    "\n",
    "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.&nbsp; Setting up your LLM üß†\n",
    "Here we'll add one extra parameter, `n_ctx`. This parameter controls the size of the input context. The larger the window, the more memory you're likely to use. The default of 512 is fine for most basic things, but as we are starting to retrieve articles and add them to the context window, we'll double the size to 1024. Feel free to play around with this number as your project needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.08 MiB, ( 4095.14 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 4224.95 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     1.55 MiB, ( 4226.50 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =     1.53 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     0.16 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(model_path = \"/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "               max_tokens = 2000,\n",
    "               temperature = 0.1,\n",
    "               top_p = 1,\n",
    "               n_gpu_layers = -1,\n",
    "               n_ctx = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test your MIstreal 7B LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6641.83 ms\n",
      "llama_print_timings:      sample time =      48.21 ms /   163 runs   (    0.30 ms per token,  3381.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6641.77 ms /     8 tokens (  830.22 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:        eval time =    8593.24 ms /   162 runs   (   53.04 ms per token,    18.85 tokens per second)\n",
      "llama_print_timings:       total time =   15837.66 ms /   170 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Science is the future, it's where we're all headed,\n",
      "With algorithms and models, predictions are guaranteed.\n",
      "We take in information from all around,\n",
      "And use it to create insights that astound.\n",
      "\n",
      "From machine learning to deep learning too,\n",
      "There's nothing that data science can't do.\n",
      "It helps us make decisions with ease,\n",
      "And find patterns that we never knew existed.\n",
      "\n",
      "With data visualization, we can see the world in a new light,\n",
      "And discover trends that were hidden from sight.\n",
      "Data science is the key to unlocking the unknown,\n",
      "And helping businesses succeed and grow.\n",
      "\n",
      "So let's embrace this future that's here to stay,\n",
      "And use data science to lead the way.\n"
     ]
    }
   ],
   "source": [
    "answer_1 = llm.invoke(\"Write a poem about data science.\")\n",
    "print(answer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.&nbsp; Retrieval Augmented Generation üîÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.&nbsp; Find your data\n",
    "Our model needs some information to work its magic! In this case, we'll be using a copy of Never Eat Alone, but feel free to swap it out for anything you like: legal documents, school textbooks, websites ‚Äì the possibilities are endless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/Never Eat Alone by Keith Ferrazzi_urdukutabkhanapk.pdf')\n",
    "pages = loader.load_and_split()\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.&nbsp; Splitting the document\n",
    "Obviously, a whole book is a lot to digest. This is made easier by [splitting](https://python.langchain.com/docs/modules/data_connection/document_transformers/) the document into chunks. You can split it by paragraphs, sentences, or even individual words, depending on what you want to analyse. In Langchain, we have different tools like the RecursiveCharacterTextSplitter (say that five times fast!) that understand the structure of text and help you break it down into manageable chunks.\n",
    "\n",
    "Check out [this website](https://chunkviz.up.railway.app/) to help visualise the splitting process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 800,\n",
    "                                               chunk_overlap = 150)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.&nbsp; Creating vectors with embeddings\n",
    "\n",
    "[Embeddings](https://python.langchain.com/docs/integrations/text_embedding) are a fancy way of saying we turn words into numbers that computers can understand. Each word gets its own unique code, based on its meaning and relationship to other words. The list of numbers produced is known as a vector. Vectors allow us to compare text and find chunks that contain similar information.\n",
    "\n",
    "Different embedding models encode words and meanings in different ways, and finding the right one can be tricky. We're using open-source models from HuggingFace, who even have a handy [leaderboard of embeddings](https://huggingface.co/spaces/mteb/leaderboard) on their website. Just browse the options and see which one speaks your language (literally!).\n",
    "> As we are doing a retrieval project, click on the `Retrieval` tab of the leaderboard to see the best embeddings for retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec225a93ec9411d882be2f72871056c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b2ca8e87344f7b8b5f921a53a5ea4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29edc6a156e54ef7886edd0fa3bfe8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52844643bb0c4268b8e40a717aa455ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1a8956a2ff4afa8b472756c1c280ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58bb82fe6614fb0823fa238546b74c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26abd3ff0c448c08a50a958115b4d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661c9916c8d9476ab77d3f72e72d82f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34349ea830147bcbb059e7121b9ff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0b750af12f418f90c94a4ad80d28b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23840e14fc242aca15be02b40430041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Define the embedding model\n",
    "embedding_model = 'sentence-transformers/all-MiniLM-l6-v2'\n",
    "\n",
    "# Get the current working directory\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "# Set the folder to store the embeddings cache\n",
    "embeddings_folder = current_folder\n",
    "\n",
    "# Create an instance of the HuggingFaceEmbeddings class\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model, cache_folder=embeddings_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03246622160077095,\n",
       " 0.048421651124954224,\n",
       " 0.10933946073055267,\n",
       " -0.007153369951993227,\n",
       " 0.00035865363315679133,\n",
       " 0.027365434914827347,\n",
       " 0.13410817086696625,\n",
       " -0.006408788729459047,\n",
       " 0.07733313739299774,\n",
       " 0.004372710362076759,\n",
       " -0.0468745157122612,\n",
       " -0.07849843800067902,\n",
       " -0.006395623553544283,\n",
       " -0.008471476845443249,\n",
       " 0.046537600457668304,\n",
       " -0.03503256291151047,\n",
       " -0.026914019137620926,\n",
       " 0.035642609000205994,\n",
       " -0.11974657326936722,\n",
       " -0.011599737219512463,\n",
       " -0.0003129113174509257,\n",
       " -0.10261186957359314,\n",
       " -0.03776707127690315,\n",
       " -0.01161273568868637,\n",
       " -0.05223803222179413,\n",
       " 0.05259639397263527,\n",
       " -0.007914486341178417,\n",
       " -0.015437936410307884,\n",
       " -0.00722751347348094,\n",
       " 0.004713333677500486,\n",
       " 0.09171861410140991,\n",
       " -0.016299452632665634,\n",
       " -0.015173033811151981,\n",
       " 0.05441003665328026,\n",
       " -0.09058492630720139,\n",
       " 0.04459471255540848,\n",
       " -0.0795581042766571,\n",
       " -0.0011238716542720795,\n",
       " 0.0236909631639719,\n",
       " -0.03680482506752014,\n",
       " 0.02214064449071884,\n",
       " -0.06234625354409218,\n",
       " -0.015554559417068958,\n",
       " 0.04259992390871048,\n",
       " 0.015803590416908264,\n",
       " -0.07372941821813583,\n",
       " 0.10614573955535889,\n",
       " 0.034202225506305695,\n",
       " 0.08875463157892227,\n",
       " 0.015194633975625038,\n",
       " -0.16160526871681213,\n",
       " 0.012264061719179153,\n",
       " -0.09900038689374924,\n",
       " 0.01854434423148632,\n",
       " 0.0229626651853323,\n",
       " 0.08019230514764786,\n",
       " 0.023165669292211533,\n",
       " -0.0014419701183214784,\n",
       " 0.04221952334046364,\n",
       " 0.04761181026697159,\n",
       " -0.025959908962249756,\n",
       " 0.11434824764728546,\n",
       " -0.014738480560481548,\n",
       " 0.06575679779052734,\n",
       " 0.03889201208949089,\n",
       " 0.019015956670045853,\n",
       " 0.00019675379735417664,\n",
       " -0.028228215873241425,\n",
       " -0.03367964178323746,\n",
       " 0.09168588370084763,\n",
       " 0.000911014387384057,\n",
       " -0.01695232465863228,\n",
       " -0.03982752189040184,\n",
       " -0.04429633542895317,\n",
       " 0.002184200333431363,\n",
       " 0.016800889745354652,\n",
       " 0.019378652796149254,\n",
       " -0.07184091955423355,\n",
       " 0.04108620807528496,\n",
       " 0.04590287059545517,\n",
       " -0.05118473619222641,\n",
       " -0.036088455468416214,\n",
       " -0.0048615396954119205,\n",
       " 0.035741548985242844,\n",
       " -0.05031854286789894,\n",
       " 0.036681365221738815,\n",
       " 0.01713470369577408,\n",
       " -0.01735401153564453,\n",
       " 0.015207100659608841,\n",
       " -0.0007733949460089207,\n",
       " -0.007820331491529942,\n",
       " -0.017690304666757584,\n",
       " -0.12432559579610825,\n",
       " -0.009647785685956478,\n",
       " 0.003309329506009817,\n",
       " -0.06058993190526962,\n",
       " -0.06399191915988922,\n",
       " 0.011620916426181793,\n",
       " -0.043455157428979874,\n",
       " 0.053316257894039154,\n",
       " 0.01420584786683321,\n",
       " 0.01345476508140564,\n",
       " 0.00984233245253563,\n",
       " 0.021365415304899216,\n",
       " -0.07884951680898666,\n",
       " 0.038243524730205536,\n",
       " -0.06901387125253677,\n",
       " 0.014018144458532333,\n",
       " 0.012577557936310768,\n",
       " 0.010962121188640594,\n",
       " -0.015479456633329391,\n",
       " -0.016767406836152077,\n",
       " -0.005627364385873079,\n",
       " -0.011153002269566059,\n",
       " 0.05792275816202164,\n",
       " -0.03647690638899803,\n",
       " 0.05567051097750664,\n",
       " -0.019040437415242195,\n",
       " 0.10403361171483994,\n",
       " 0.04294233024120331,\n",
       " -0.02395367994904518,\n",
       " 0.030164001509547234,\n",
       " 0.07960356771945953,\n",
       " 0.020409539341926575,\n",
       " -0.08830194175243378,\n",
       " -0.017238594591617584,\n",
       " 0.054107535630464554,\n",
       " -5.468807469628223e-33,\n",
       " 0.053952742367982864,\n",
       " -0.1190781369805336,\n",
       " 0.07446084916591644,\n",
       " 0.043795935809612274,\n",
       " -0.13455601036548615,\n",
       " -0.0064704748801887035,\n",
       " 0.027104124426841736,\n",
       " -0.01616767607629299,\n",
       " -0.007558579556643963,\n",
       " 0.06641349196434021,\n",
       " 0.012818068265914917,\n",
       " -0.031121980398893356,\n",
       " -0.005838119890540838,\n",
       " -0.025567954406142235,\n",
       " 0.04878920689225197,\n",
       " -0.07131598144769669,\n",
       " -0.10789376497268677,\n",
       " -0.11302351951599121,\n",
       " 0.0029279182199388742,\n",
       " 0.07832510769367218,\n",
       " 0.01170101948082447,\n",
       " -0.05632202699780464,\n",
       " 0.01379562821239233,\n",
       " -0.05153311416506767,\n",
       " 0.018650256097316742,\n",
       " -0.09195214509963989,\n",
       " -0.03370005264878273,\n",
       " -0.024588676169514656,\n",
       " 0.02956290915608406,\n",
       " 0.04920799657702446,\n",
       " -0.06541617214679718,\n",
       " 0.07449125498533249,\n",
       " -0.03083086758852005,\n",
       " -0.043636616319417953,\n",
       " -0.00309186615049839,\n",
       " -0.00037453757249750197,\n",
       " -0.11711706966161728,\n",
       " 0.013738051056861877,\n",
       " 0.06523347645998001,\n",
       " -0.020332353189587593,\n",
       " 0.04570893570780754,\n",
       " -0.04829491674900055,\n",
       " -0.046332947909832,\n",
       " 0.0005500693805515766,\n",
       " 0.03650614246726036,\n",
       " 0.08617543429136276,\n",
       " -0.001201758161187172,\n",
       " 0.037234898656606674,\n",
       " -0.007840101607143879,\n",
       " -0.05356508865952492,\n",
       " 0.0062956553883850574,\n",
       " -0.04556344076991081,\n",
       " 0.05649267137050629,\n",
       " -0.006638159975409508,\n",
       " -0.0324333980679512,\n",
       " -0.02299618162214756,\n",
       " -0.007636041380465031,\n",
       " 0.04390164837241173,\n",
       " 0.02908916212618351,\n",
       " 0.0003140554472338408,\n",
       " -0.004439322277903557,\n",
       " -0.08549115061759949,\n",
       " -0.032490074634552,\n",
       " 0.02014140971004963,\n",
       " 0.010941495187580585,\n",
       " -0.03481823951005936,\n",
       " 0.0711389109492302,\n",
       " -0.04634728655219078,\n",
       " 0.007393505424261093,\n",
       " -0.004073277115821838,\n",
       " -0.03104332461953163,\n",
       " 0.006394125055521727,\n",
       " -0.011879894882440567,\n",
       " -0.020631296560168266,\n",
       " -0.0693865492939949,\n",
       " -0.06582732498645782,\n",
       " 0.06924860924482346,\n",
       " -0.02648175321519375,\n",
       " 0.032770052552223206,\n",
       " -0.019663583487272263,\n",
       " 0.013331376016139984,\n",
       " 0.08289453387260437,\n",
       " -0.03497115895152092,\n",
       " -0.004354111384600401,\n",
       " 0.0820302739739418,\n",
       " -0.0684376209974289,\n",
       " -0.020825179293751717,\n",
       " -0.023663027212023735,\n",
       " 0.0008036186918616295,\n",
       " 0.04473935812711716,\n",
       " 0.022357024252414703,\n",
       " -0.0022531484719365835,\n",
       " -0.0009343428537249565,\n",
       " -0.1542075127363205,\n",
       " -0.039835184812545776,\n",
       " 4.058307920847559e-33,\n",
       " 0.06638874858617783,\n",
       " 0.01167936809360981,\n",
       " 0.05636944994330406,\n",
       " 0.11340698599815369,\n",
       " -0.06917641311883926,\n",
       " -0.08733119815587997,\n",
       " -0.0240499097853899,\n",
       " 0.006496944930404425,\n",
       " 0.0579831600189209,\n",
       " 0.08077934384346008,\n",
       " -0.0036902278661727905,\n",
       " -0.06433931738138199,\n",
       " 0.21867237985134125,\n",
       " -0.03715752065181732,\n",
       " -0.05065136030316353,\n",
       " -0.006874991115182638,\n",
       " 0.029888112097978592,\n",
       " 0.04477430507540703,\n",
       " -0.03335633501410484,\n",
       " 0.037434037774801254,\n",
       " -0.017677366733551025,\n",
       " 0.08417800813913345,\n",
       " -0.03150034695863724,\n",
       " 0.03151562437415123,\n",
       " -0.05838618054986,\n",
       " -0.055715639144182205,\n",
       " 0.048844750970602036,\n",
       " 0.008411998860538006,\n",
       " -0.003509954083710909,\n",
       " -0.11579035222530365,\n",
       " -0.021728390827775,\n",
       " -0.003994788508862257,\n",
       " -0.0376625657081604,\n",
       " -0.0153809180483222,\n",
       " 0.015878550708293915,\n",
       " 0.005110965576022863,\n",
       " -0.04046955704689026,\n",
       " 0.04044565185904503,\n",
       " -0.04475035518407822,\n",
       " 0.011412130668759346,\n",
       " -0.035474203526973724,\n",
       " -0.044552870094776154,\n",
       " -0.02530253492295742,\n",
       " 0.014117705635726452,\n",
       " 0.049550723284482956,\n",
       " -0.08939160406589508,\n",
       " 0.04093997925519943,\n",
       " -0.0016816970892250538,\n",
       " 0.033755820244550705,\n",
       " 0.04327751323580742,\n",
       " -0.036282654851675034,\n",
       " -0.011579953134059906,\n",
       " -0.03255440667271614,\n",
       " 0.04285557568073273,\n",
       " 0.13238538801670074,\n",
       " 0.0015879187267273664,\n",
       " 0.1212991327047348,\n",
       " 0.0009635486640036106,\n",
       " 0.012493696995079517,\n",
       " -0.030641522258520126,\n",
       " -0.009881324134767056,\n",
       " 0.025203034281730652,\n",
       " 0.022647079080343246,\n",
       " 0.09096010029315948,\n",
       " 0.030991369858384132,\n",
       " 0.054170895367860794,\n",
       " -0.015782460570335388,\n",
       " 0.026622600853443146,\n",
       " -0.051616981625556946,\n",
       " 0.010315471328794956,\n",
       " -0.05139238387346268,\n",
       " 0.004256694111973047,\n",
       " 0.06318705528974533,\n",
       " -0.023067843168973923,\n",
       " 0.006244179327040911,\n",
       " -0.047649700194597244,\n",
       " -0.023729318752884865,\n",
       " -0.019603680819272995,\n",
       " -0.0063459970988333225,\n",
       " -0.04194716736674309,\n",
       " 0.03584890440106392,\n",
       " 0.09224901348352432,\n",
       " 0.004643287044018507,\n",
       " -0.022417742758989334,\n",
       " -0.1011442318558693,\n",
       " 0.023178672417998314,\n",
       " 0.04211604967713356,\n",
       " 0.02535773441195488,\n",
       " -0.05539859086275101,\n",
       " -0.0835457444190979,\n",
       " -0.07377435266971588,\n",
       " 0.00033033048384822905,\n",
       " -0.05012992024421692,\n",
       " -0.05230417475104332,\n",
       " 0.05312104895710945,\n",
       " -1.6271926028821326e-08,\n",
       " -0.03265082836151123,\n",
       " -0.01795642077922821,\n",
       " -0.013527780771255493,\n",
       " -0.042042750865221024,\n",
       " -0.002997748088091612,\n",
       " 0.003070527222007513,\n",
       " 0.001990324817597866,\n",
       " -0.09985331445932388,\n",
       " -0.010785074904561043,\n",
       " -0.05038928985595703,\n",
       " 0.027058036997914314,\n",
       " -0.0146017549559474,\n",
       " 0.04473709687590599,\n",
       " 0.05581502243876457,\n",
       " 0.06714364886283875,\n",
       " -0.015049722045660019,\n",
       " 0.1243615671992302,\n",
       " 0.0831354409456253,\n",
       " -0.03990031033754349,\n",
       " 0.005377491470426321,\n",
       " 0.03416360542178154,\n",
       " 0.04555779695510864,\n",
       " 0.05563720315694809,\n",
       " 0.014093738980591297,\n",
       " -0.01659914292395115,\n",
       " 0.10182202607393265,\n",
       " -0.0114979799836874,\n",
       " -0.03196525573730469,\n",
       " -0.03450135514140129,\n",
       " 0.1231444925069809,\n",
       " 0.06397794187068939,\n",
       " -0.030220970511436462,\n",
       " -0.01074135210365057,\n",
       " -0.04622090235352516,\n",
       " 0.026808880269527435,\n",
       " 0.004843328148126602,\n",
       " -0.006288276519626379,\n",
       " 0.012496042996644974,\n",
       " -0.011477441526949406,\n",
       " -0.006064353045076132,\n",
       " 0.023351818323135376,\n",
       " 0.06879075616598129,\n",
       " 0.03978113830089569,\n",
       " -0.02351124957203865,\n",
       " -0.03653401508927345,\n",
       " 0.03841272369027138,\n",
       " 0.020324647426605225,\n",
       " -0.024913012981414795,\n",
       " -0.0008210959495045245,\n",
       " 0.008146406151354313,\n",
       " -0.027789698913693428,\n",
       " -0.002834062557667494,\n",
       " -0.04441450163722038,\n",
       " 0.06262453645467758,\n",
       " -0.004760414361953735,\n",
       " 0.011714635416865349,\n",
       " 0.014699353836476803,\n",
       " 0.017748747020959854,\n",
       " 0.024494227021932602,\n",
       " -0.025931714102625847,\n",
       " 0.09070852398872375,\n",
       " 0.09178222715854645,\n",
       " 0.029576655477285385,\n",
       " -0.06361748278141022]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = 'How to love youself?'\n",
    "query_result = embeddings.embed_query(test_text)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 20' character sentence was transformed into a 384 dimension vector\n"
     ]
    }
   ],
   "source": [
    "characters = len(test_text)\n",
    "dimensions = len(query_result)\n",
    "print(f\"the {characters}' character sentence was transformed into a {dimensions} dimension vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding vectors have a fixed length, meaning each vector produced by this specific embedding will always have 384 dimensions. Choosing the appropriate embedding size involves a trade-off between accuracy and computational efficiency. Larger embeddings capture more semantic information but require more memory and processing power. Start with the provided MiniLM embedding as a baseline and experiment with different sizes to find the optimal balance for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.&nbsp; Creating a vector database\n",
    "Imagine a library where books aren't just filed alphabetically, but also by their themes, characters, and emotions. That's the magic of vector databases: they unlock information beyond keywords, connecting ideas in unexpected ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vector_db2 = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db2.save_local('/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/faiss_index2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also search your database to see which vectors are close to your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Never Eat Alone 97 \\na run. As a makeshift staff meeting, I\\'ll occasionally ask a few \\nemployees to share a car ride with me to the airport. I figure out \\nways to as much as triple my active working day through such \\nmultitasking. And, in the process, I\\'m connecting people from dif\\xad\\nferent parts of my \"community.\" \\nThe more new connections you establish, the more opportuni\\xad\\nties you\\'ll have to make even more new connections. As Robert \\nMetcalfe, the inventor of Ethernet, says: The value of a network \\ngrows proportional to the square of the number of its users. In the \\ncase of the Internet, every new computer, every new server, and \\nevery new user added expands the possibilities for everyone else \\nwho\\'s already there. The same principle holds true in growing', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/Never Eat Alone by Keith Ferrazzi_urdukutabkhanapk.pdf', 'page': 107}),\n",
       " Document(page_content=\"There are a number of things that you can do to harness the \\npower of your preexisting network. Have you investigated the \\nfriends and contacts of your parents? How about your siblings? \\nYour friends from college and grad school? What about your \\nchurch, bowling league, or gym? How about your doctor or lawyer \\nor realtor or broker? \\nIn business, we often say that your best customers are the cus\\xad\\ntomers you have now. In other words, your most successful sales \\nleads come from the selling you've already done. The highest \\nreturns don't come from new sales; they come on top of the cus\\xad\\ntomer base you've already established. It's easiest to reach out to \\nthose people who are at least tangentially part of your network. \\nThe big hurdles of networking revolve around the cold calls,\", metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/Never Eat Alone by Keith Ferrazzi_urdukutabkhanapk.pdf', 'page': 56}),\n",
       " Document(page_content=\"46 Never Eat Alone \\nIf you're like most people, you see a tiny parcel of cleanly cut \\ngrass that represents the usual suspects of an ordinary Rolodex. It \\nconsists of your immediate friends, coworkers, and business part\\xad\\nners: the most obvious people. \\nYour real network, however, is an overgrown jungle with an \\ninfinite variety of hidden nooks and crannies that are being neg\\xad\\nlected. \\nYour potential for connecting is at this moment far bigger than \\nyou realize. All around you are golden opportunities to develop \\nrelationships with people you know, who know people you don't \\nknow, who know even more people. \\nThere are a number of things that you can do to harness the \\npower of your preexisting network. Have you investigated the\", metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/Never Eat Alone by Keith Ferrazzi_urdukutabkhanapk.pdf', 'page': 56}),\n",
       " Document(page_content='time and energy in building personal relationships with the \\nright people will pay dividends. The majority of \"one percenters,\" \\nas I call the ultra-rich and successful whom many of my mentees \\naspire toward, are one percenters because they understand this \\ndynamic‚Äîbecause, in fact, they themselves used the power of \\ntheir network of contacts and friends to arrive at their present \\nstation. \\nBut to do so, first you have to stop keeping score. You can\\'t \\namass a network of connections without introducing such con\\xad\\nnections to others with equal fervor. The more people you help, \\nthe more help you\\'ll have and the more help you\\'ll have helping \\nothers. It\\'s like the Internet. The more people who have access, and \\nuse it, the more valuable the Internet becomes. I now have a small', metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/Never Eat Alone by Keith Ferrazzi_urdukutabkhanapk.pdf', 'page': 26})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db2.similarity_search('how to grow your network?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever \n",
    "retriever = vector_db2.as_retriever(search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key='chat_history',\n",
    "                                  return_messages=True,\n",
    "                                  output_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Prompt\n",
    "from langchain import PromptTemplate\n",
    "# prompt\n",
    "template = \"\"\"\n",
    "<s> [INST]\n",
    "You are polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
    "\n",
    "In your response, PLEASE ALWAYS:\n",
    "  (0) Be a career Analyst: give precise answers how one person can develop his career\n",
    "  (1) Provide examples of other people: how growing network has been helpful for them\n",
    "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
    "  (3) Do not make answers too boring, add some humor with your answer\n",
    "[/INST]\n",
    "[INST]\n",
    "Answer the following question using the context provided.\n",
    "The question is surrounded by the tags <q> </q>.\n",
    "The context is surrounded by the tags <c> </c>.\n",
    "<q>\n",
    "{question}\n",
    "</q>\n",
    "<c>\n",
    "{context}\n",
    "</c>\n",
    "[/INST]\n",
    "</s>\n",
    "[INST]\n",
    "Helpful Answer:\n",
    "[INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template,\n",
    "                        input_variables = ['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chain = ConversationalRetrievalChain.from_llm(llm,\n",
    "                                              retriever=retriever,\n",
    "                                              memory=memory,\n",
    "                                              return_source_documents=True,\n",
    "                                              combine_docs_chain_kwargs={'prompt':prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6641.83 ms\n",
      "llama_print_timings:      sample time =     107.21 ms /   378 runs   (    0.28 ms per token,  3525.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22101.90 ms /   643 tokens (   34.37 ms per token,    29.09 tokens per second)\n",
      "llama_print_timings:        eval time =   21942.85 ms /   377 runs   (   58.20 ms per token,    17.18 tokens per second)\n",
      "llama_print_timings:       total time =   45961.06 ms /  1020 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'how to make friends?',\n",
       " 'chat_history': [HumanMessage(content='how to make friends?'),\n",
       "  AIMessage(content=\"To make friends with little or no fear, you can start by learning to speak confidently and effectively. This will help you overcome shyness and feel more comfortable in social situations. You can also try joining groups or clubs that share your interests, such as a book club or a hobby group. This will give you an opportunity to meet people who are passionate about the same things as you and make it easier to start conversations. Additionally, you can ask for help from others who have experience making friends, such as colleagues or friends of friends. They may be able to offer advice and support as you work on building your social skills.\\n\\nHere are some examples of how growing a network has been helpful for others:\\n\\n* John was feeling lonely and isolated at work, but he joined a book club and met several people who shared his love of reading. Over time, he made close friends with some of the members and even started dating one of them.\\n* Sarah was nervous about making new friends in her new city, but she joined a hiking group and found that she had a lot in common with the other members. She made several friends and even started volunteering with the group, which helped her feel more connected to her community.\\n* Tom was struggling to make friends in his new job, but he reached out to a colleague who had experience making friends and asked for advice. The colleague introduced him to a few people in the office who were also interested in socializing, and Tom was able to make some new friends and feel more comfortable in his new environment.\\n\\nRemember that building relationships takes time and effort, so be patient with yourself and don't give up if things don't go as planned. Keep practicing your social skills and seeking out opportunities to meet new people, and you'll eventually find the right connections.\")],\n",
       " 'answer': \"To make friends with little or no fear, you can start by learning to speak confidently and effectively. This will help you overcome shyness and feel more comfortable in social situations. You can also try joining groups or clubs that share your interests, such as a book club or a hobby group. This will give you an opportunity to meet people who are passionate about the same things as you and make it easier to start conversations. Additionally, you can ask for help from others who have experience making friends, such as colleagues or friends of friends. They may be able to offer advice and support as you work on building your social skills.\\n\\nHere are some examples of how growing a network has been helpful for others:\\n\\n* John was feeling lonely and isolated at work, but he joined a book club and met several people who shared his love of reading. Over time, he made close friends with some of the members and even started dating one of them.\\n* Sarah was nervous about making new friends in her new city, but she joined a hiking group and found that she had a lot in common with the other members. She made several friends and even started volunteering with the group, which helped her feel more connected to her community.\\n* Tom was struggling to make friends in his new job, but he reached out to a colleague who had experience making friends and asked for advice. The colleague introduced him to a few people in the office who were also interested in socializing, and Tom was able to make some new friends and feel more comfortable in his new environment.\\n\\nRemember that building relationships takes time and effort, so be patient with yourself and don't give up if things don't go as planned. Keep practicing your social skills and seeking out opportunities to meet new people, and you'll eventually find the right connections.\",\n",
       " 'source_documents': [Document(page_content=\"with little or no fear. If you're not yet ready to take the big leap of \\naddressing new people on your own, let these people help you and \\nshow you the way. Take them with you, when appropriate, to \\nsocial outings and observe their behaviors. Pay attention to their \\nactions. Over time, you'll adopt some of their techniques. Slowly, \\nyou'll build up the courage to reach out by yourself. \\n‚Ä¢ Learn to speak. \\nMany businesses have responded to the nearly infinite number \\nof people who recognize they need to become better speakers. \\nThese educational organizations realize you're not looking to give \\nspeeches to an audience of a thousand people (at least initially). \\nMost people who come to them for help are looking to gain \\nself-confidence and some trusty tools for overcoming shyness.\", metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/Never Eat Alone by Keith Ferrazzi_urdukutabkhanapk.pdf', 'page': 62}),\n",
       "  Document(page_content=\"You've just paid a boatload of money to be at this conference \\n(unless you're a speaker, when it's usually free!), and you want to \\nmeet as many people as you can in the time that you have. You're \\nnot looking to make a best friend. You are looking, however, to \\nmake enough of a connection to secure a follow-up. \\nCreating a connection between any two people necessitates a \\ncertain level of intimacy. In two minutes, you need to look deeply \\ninto the other person's eyes and heart, listen intently, ask questions \\nthat go beyond just business, and reveal a little about yourself in \\na way that introduces some vulnerability (yes, vulnerability; it's \\ncontagious!) into the interaction. All these things come together \\nto create a genuine connection.\", metadata={'source': '/Users/sadiakhanrupa/Bootcamp Main Phase/Chapter_8_generative_Ai/Chapter_8_generative_Ai/Model/faiss_index/Never Eat Alone by Keith Ferrazzi_urdukutabkhanapk.pdf', 'page': 131})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('how to make friends?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6641.83 ms\n",
      "llama_print_timings:      sample time =       4.86 ms /    13 runs   (    0.37 ms per token,  2672.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12471.70 ms /   439 tokens (   28.41 ms per token,    35.20 tokens per second)\n",
      "llama_print_timings:        eval time =     663.92 ms /    12 runs   (   55.33 ms per token,    18.07 tokens per second)\n",
      "llama_print_timings:       total time =   13385.66 ms /   451 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6641.83 ms\n",
      "llama_print_timings:      sample time =      81.39 ms /   227 runs   (    0.36 ms per token,  2788.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18574.25 ms /   653 tokens (   28.44 ms per token,    35.16 tokens per second)\n",
      "llama_print_timings:        eval time =   12986.82 ms /   226 runs   (   57.46 ms per token,    17.40 tokens per second)\n",
      "llama_print_timings:       total time =   33131.76 ms /   879 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To improve your social skills, you can start by learning how to speak confidently in different situations. This can be done through various educational organizations that offer courses on public speaking and overcoming shyness. Additionally, observing and learning from others who are skilled at making small talk and building relationships can also help you develop your own social skills.\n",
      "\n",
      "For example, the book \"Never Eat Alone\" by Keith Ferrazzi highlights the importance of face-to-face communication in building relationships. The author emphasizes that while digital communication may be efficient, it is not effective when it comes to making friends. He suggests that conversation is an acquired skill and can be learned with determination and proper information.\n",
      "\n",
      "Another example is the CEOs who take pride in their ability to negotiate social situations with relative ease. These individuals have likely developed their social skills through years of practice and observation, and they can provide valuable insights into how to make successful small talk and build relationships.\n",
      "\n",
      "Overall, improving your social skills takes time and effort, but it is definitely possible with the right mindset and resources.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke('how to make friends?')['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6641.83 ms\n",
      "llama_print_timings:      sample time =       5.17 ms /    14 runs   (    0.37 ms per token,  2706.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19548.63 ms /   686 tokens (   28.50 ms per token,    35.09 tokens per second)\n",
      "llama_print_timings:        eval time =     739.13 ms /    13 runs   (   56.86 ms per token,    17.59 tokens per second)\n",
      "llama_print_timings:       total time =   20713.55 ms /   699 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6641.83 ms\n",
      "llama_print_timings:      sample time =     135.41 ms /   360 runs   (    0.38 ms per token,  2658.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18795.86 ms /   658 tokens (   28.57 ms per token,    35.01 tokens per second)\n",
      "llama_print_timings:        eval time =   20735.70 ms /   359 runs   (   57.76 ms per token,    17.31 tokens per second)\n",
      "llama_print_timings:       total time =   42019.67 ms /  1017 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a data scientist, finding job opportunities in Germany can be challenging, but with the right approach, it's possible to land your dream job. Here are some tips to help you get started:\n",
      "\n",
      "1. Networking: Building a strong network is crucial for finding job opportunities as a data scientist. Attend industry events, join professional organizations, and connect with other professionals in the field on LinkedIn. You can also reach out to recruiters and hiring managers directly to learn more about open positions.\n",
      "2. Online Job Boards: There are many online job boards that specialize in data science jobs in Germany. Some popular options include Glassdoor, Indeed, and LinkedIn Jobs. These platforms allow you to search for job postings based on your skills and experience.\n",
      "3. Personal Branding: Developing a strong personal brand can help you stand out from other candidates and increase your chances of landing a job. This can include creating a professional website or blog, publishing articles in industry publications, and speaking at conferences or meetups.\n",
      "4. Internships and Freelance Work: Consider taking on internships or freelance work to gain experience and build your portfolio. This can also help you network with professionals in the field and potentially lead to full-time job opportunities.\n",
      "5. Continuing Education: Staying up-to-date with the latest technologies and techniques in data science is essential for success in the field. Consider taking courses or earning certifications to demonstrate your expertise and increase your value as a candidate.\n",
      "\n",
      "Remember, finding a job as a data scientist takes time and effort. Be persistent and don't give up on your search. With the right approach, you can find a rewarding career in this exciting field.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke('how I can get a job as Data Scientist in germany')['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
